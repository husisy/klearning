# cs231n

## lecture 1, introduction to convolutional neural networks for visual recognition

1. face detection
2. SIFT object recognition David Lowe, 1999
3. spatial pyramid matching
4. HoG
5. deformable part model
6. object recognition / PASCAL visual object challenge
7. ImageNet
8. WorldNet
9. CNN
10. AlexNet
11. GoogleNet, VGG net
12. "deep learning" by Goodfellow

## lecture2 image classification

1. occlusioin, viewpoint variation, illumination, deformation, background clutter, intraclas variation
2. k-Nearest Neighbor: k distance metrix
   * train fast, test / deploy slow
   * distance metric: L1 (Manhattan), L2 (Euclidean)
   * hyperparameter: k, distance metric
   * never used: slow at test time, distance metrics not informative, curse of dimensionality
3. cross-validation: seldomly used in deep learning

## lecture 3 loss function and optimization

1. loss function
2. multiclass SVM loss: Hinge loss
3. softmax classifier (multinomial logistic regression)
4. regularization
   * L2 MAP inference, Gaussian prior (Bayesian)
   * L1
   * Elastic net (L1+L2)
   * max norm regularization
   * dropout
   * batch normalization
   * stochastic depth
5. image feature
   * color histogram
   * Histogram of Oriented Gradients (HoG)
   * bag of words

## lecture 4 introduction to neural networks

1. neural turing machine
2. back propagation
   * add gate: gradient distributor
   * max gate: gradient router
   * multiply gate: gradient switcher

## lecture 5 convolutional networks

1. Frank Rosenblatt: Mark I perceptron
2. Widrow and Hoff: Adaline/Madaline
3. Rumelhart et al. : back propagation
4. Geoff Hinton and Ruslan Salakhutdinov (2006): deep neural network, pre-trained
5. Fukushima: Neurocognitron, pooling
6. Krizhevsky, Sutskever, Hinton: ImageNet, AlexNet
7. ConvNets
   * classification
   * Retrieval
   * Detection: Faster R-CNN
   * Segmentation
   * self driving cars
   * face recognition
   * pose recognition
   * Image captioning
   * deep dream, art work
8. number of parameter: bias, depth
9. channel usually be power of 2,
10. $1\times 1$ convolution
11. pooling, downsampling

## lecture 6 training neural networks I

1. setup
   * activation functions
   * preprocessing
   * weight initilaization
   * regularization
   * gradient check
2. training dynamics
   * babysitting the learning process
   * parameter updates
   * hyperparameter optimization
3. evaluation
   * model ensembles
4. sigmoid
   * saturated neurons "kill" off the gradient
   * output not zero-centered
   * compute expensive
5. tanh
6. ReLU
   * not saturate
   * computationally efficient
   * converge faster than sigmoid/tanh in practice (e.g. 6x)
   * more biologically plausible
   * not zero-centered
   * dead neuron
   * initialize ReLU with slightly positive biases
7. Leaky ReLU (Mass et al. 2013; He et al. 2015)
   * not saturate
   * conputationally efficient
   * converge faster
   * will not "die"
8. PReLU (parametric Rectifier)
   * $max \left( \alpha x, x \right)$
9. exponeitial linear units (ELU)
   * $x\quad if \; x>0$
   * $\alpha \left( e^x-1 \right) \quad if \; x<0$
   * all benefits of ReLU
   * closer to zero mean outputs
   * nagative saturation regime, add some rebustness to noise
10. maxout newron, Goodfellow et al. 2013
    * not have the basic form of dot product -> nonlinearity
    * generalize ReLU and Leaky ReLU
    * linear regime, not saturate, not die
    * double the number of parameters
11. in practice
    * use ReLU, be careful with learning rate
    * try out Leaky ReLU / maxout / ELU
    * try out tanh but don't expect too much
    * do not use sigmoid
12. data prepocessing
    * zero-center: AlexNet (mean image=[32,32,3]array), VggNet (mean along each channel=[1,1,3]array)
    * normalize (not use)
    * PCA, whitening (not use)
13. weight initalization
    * random from Gaussian distribution with small $\sigma=0.01$: all activations become zero, gradients are small
    * random from Gaussian distribution with big $\sigma=1$: all activations saturate
    * Xavier initialization, Glorot et al. 2010: assume linear activation, not suit for ReLU
    * He initialization, He et al. 2015: ReLU
    * MSRA initialization
    * [others] All you need is a good init, Mishkin and Matas 2015
    * [others] Data-dependent initialization of Convolution neural network, Krahenbuhl et al. 2015
    * [others] Delving deep into rectifiers: surpassing human-level performance on IImageNet classification, He et al. 2015
14. batch normalization, loffe and Szegedy, 2015
    * inserted after Fully Connected or Convolutional layers and before nonlinearity
    * improve gradient flow through the network
    * allow higher learning rate
    * reduce the strong dependence on initialization
    * a form of regularization in a funny way, and slightly reduce the need for dropout
    * loss will be less sensitive to perturbation of $W$
15. check before training: loss will go down to zero for small amount of data (overfit easily)
16. hyperparameter optimization
    * only a few epochs to get rough idea of what params work
    * long running time, finer search

## lecture 7 training neural networks II

1. stochastic gradient descent (low dimension - high dimension)
   * local minima: severe - okay
   * saddle point: okay - severe
2. **vanilla** SGD
3. SGD + momentum
4. nesterov accelerated gradient
5. sharp minima could be a minima that overfits, a feature of SGD
6. AdaGrad, John Duchi, dead at saddle point, bias correction
7. RMSProp, bias correction
8. Adam, bias correction
9. tilted taco problem still hard to addressed
10. learning rate decay
    * seconda order hyperparameter
    * common with SGD, seldomly used with Adam
11. second-order optimization
    * no learning rate
    * Newton method / Quasi-Newton method (BGFS, L-BFGS)
    * not work well with non-convex
    * not work well with stochastic (mini-batch)
    * L-BFGS works in style transfer (less stochastic and fewer parameter)
12. *regularization*
    * training: add random noise
    * testing: marginalize over the noise
13. model ensembles (machine learning)
    * train multiple independent models
    * at test time average their result
    * (deep learning) snapshots of a single model during training
    * Polyak averaging: keep a moving average of the parameter vector and use that at test time
14. L2 regularization maybe doesn't make a lot of sense in the context.
15. dropout
    * units in fully connected layer
    * channels in ConvNet
    * prevent co-adaptation of feature
    * no stochasticity in test
16. data augmentation
    * flip, crop, color Jittering, translation, rotation, strentching, shearing, lens distortions
    * apply PCA to all [R,G,B] pixels in training set
    * sample a 'color offset' along principal component direction
    * add offset to all pixel o a training image
    * during testing: average a fixed set of crop
17. DropConnect
18. fractional max pooling
19. stochastic depth
20. transfer learning

## lecture 8 deep learning software

1. CPU/GPU
   * Intel vs AMD
   * NVIDIA vs AMD, graphics card
   * vim vs Emacs, text editor
2. CUDA (NVIDIA only)
   * write C-like code that runs directly on the GPU
   * higher-level APIs: cuBLAS (matrix operation), cuFFT, cuDNN (convolution, forward, backward, batch normalization, recurrent network)
3. OpenCL
4. Udacity: intro to Parallel Programming
5. software
   * Caffe (UC Berbeley) -> Caffe2 (Facebook)
   * Torch (NYU / Facebook) -> PyTorch (Facebook)
   * Theano (U Montreall) -> Tensorflow (Google)
   * Paddle (baidu)
   * CNTK (Microsoft)
   * MXNet (Amazon)
6. tensorflow
   * dummy node
7. Tensorflow high-level wrappers
   * Keras
   * TFLearn
   * TensorLayer
   * tf.layers
   * TF-Slim
   * tf.contrib.learn
   * Pretty Tensor
   * Sonnet
8. Tensorboard
9. TensorFlow: distributed version
10. PyTorch
    * tensor: imperative ndarray, but runs on GPU
    * variable: node in a computational graphs, store data and gradient
    * module, a neural network layer, may store state and learnable weights
    * ```torch.FloatTensor```, ```torch.cuda.FloatTensor```
    * ```required_grad```
    * new autograd functions: ```forward()```, ```backward()```
    * wrapper: nn, keras
    * dataloader
    * pretrained models
    * Visdom (TensorBoard)
    * static (TensorFlow) vs dynamic graphs
    * serialization: static
    * conditional: dynamic
    * loops/recurrent networks/recursive network/modular network: dynamic [TensorFlow Fold make dynamic graphs easier in TensorFlow through dynamic batching, ICLR 2017, Deep Learning with Dynamic Computaion Graphs]
11. Torch vs PyTorch
    * Lua vs Python
    * no autograd vs autograd
    * more stable vs newer, still changing
    * lots of existing code vs less existing code
    * fast vs fast
12. Caffe
    * core written in C++
    * has Python and MATLAB bindings
    * good fro training or finetuning, feedforward classification models
    * often no need to write code
    * not used as in research anymore, still popular for deploying models
    * define network (prototxt)
    * can deploy without python
    * need to write c++/CUDA for new GPU layers
    * not good for recurrent networks
    * cumbersome for big networks
13. Caffe2
    * new, 2017
    * static graphs
    * core written in C++
    * nice python interface
    * train model in python, serialize and deploy without python
    * works on ios/android
14. comparison
    * tensorflow: not perfect, huge community, wide usage, high-level wrapper (Keras, Sonnet), distributed version,
    * PyTorch: research, rough patches
    * production deployment: Caffe, Caffe2, Tensorflow
    * mobile: TensorFlow, Caffe2

## lecture 9 CNN architectures

1. SEE ```CNN Development```
2. size: ```n2 = (n1 - k + 2*p)/s + 1```
3. ImageNet Large Scale Visual Recognition Challenge (ILSVRC) winners
   * 2012 AlexNet (8), 16.4%
   * 2013 ZFNet (8) (Zeller Fergus Net) 11.7%, imporoved hyperparameter over AlexNet, same architecture, same number of layers,
   * 2014 VGG19 (19) 7.3
   * 2014 GoogleNet (22) 6.7
   * 2015 ResNet (152) (Oxford) 3.57, small filters, deeper networks
4. vanilla

ZFNet

```bash
AlexNet but
conv1: 11*11 filter stride4 -> 7*7 filter stride2
conv3,4,5: 384,384,256 -> 512,1024,512
```

GoogleNet

1. 22 layers
2. efficient inception module
3. no FC layers
4. only 5 million parameters
5. inception module
   * bottleneck layer: conv$1\times1$ reduce expensive conputate
6. auxiliary classification output to inject additional gradient at lower layers
7. Network in Network (NiN) [Lin et al. 2014]
   * Mlpconv layer with "micronetwork" within each conv layer to compute more abstract features for local patches
   * Micronetwork uses multilayer perceptron (FC ie $1\times 1$ conv layers)
   * Precursor to GoogLeNet and ResNet bottlenect layers
   * Philosophical inspiration for GooLeNet

ResNet

1. 152 layers
2. swept all classification and detection competitions in ILSVRC15 and COCO15
   * ImageNet Classification "Ultra-deep" 152-layer nets
   * ImageNet Detection 16% better than 2nd
   * ImageNet Localization 27% better than 2nd
   * COCO Detection 11% better than 2nd
   * COCO Segmentation 12% better than 2nd
3. hypothesis: the problem is an optimization problem, deeper models are harder to optimize
4. the deeper model should be able to perform at least as well as the shallower model
5. use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping
6. stack residual blocks
7. every residual block has two $3\times 3$ conv layers
8. periodically, double number of filters and downsample spatially using stride2
9. additional conv layer at the beginning
10. no FC layers at the end (only FC 1000 to ouput classes)
11. for deeper ResNet, use bottleneck layer to improve efficiency (GoogLeNet)
12. batch normalization after every conv layer
13. He initialization
14. SGD + momentum (0.9)
15. learning rate 0.1, divided by 10 while validation error plateaus
16. mini-batch size 256
17. weight decay of 1e-5
18. no dropout used
19. **improving Resnets**
20. Identity Mappings in Deep Residual Networks [He et al. 2016]
    * creates a more direct path for propagatin information throughout network (moves activation to residual mapping pathway)
21. Wide Residual Networks [Zagoruyko et al. 2016]
    * argues that residuals are the important factor, not depth
    * use wider residual blocks ($F\times k$ filters instead of F Filters in each layer)
    * 50-layer wide ResNet outperforms 152-layer original ResNet
    * increasing width instead of depth more computationally efficient (parallelizable)
22. Aggregated Residual Transformations for Deep Neural Networks (ResNeXt) [Xie et al. 2016]
    * increase width of residual block through parallel pathways "cardinality"
    * parallel pathways similar in spirit to Inception module
23. Deep Networks with Stochastic Depth [Huang et al. 2016]
    * motivation: reduce vanishing gradients and training time through short networks during training
    * randomly drop a subset of layers during each training pass
    * bypass with identity function
    * use full deep network at test time
24. **beyond ResNets**
25. FractalNet: Ultra-Deep Neural Networks without Residuals [Larsson et al. 2017]
    * argues that key is transitioning effectively from shallow to deep and residual representations are not necessary
    * fractal architecture with both shallow and deep paths to output
    * trained with dropping out sub-paths
    * full network at test time
26. Densely Connected Convolutional Networks [Huang et al. 2017]
    * Dense blocks where each layer is connected to every other layer in feedforward fashion
    * alleviates vanishing gradient, strengthens feature propagation, encourages feature reuse
27. SqueezeNet: AlexNet-level accuracy with 50x Fewer Parameters and <0.5Mb Model Size [landola et al. 2017]
    * Fire modules consisting of squeeze layer with $1\times 1$ filters feeding an expand layer with $1\times 1$ and $3\times 28 filters
    * AlexNet level accuracy on ImageNet with 50x fewer parameters
    * compress to 510x smaller than AlexNet

## lecture 10 recurrent neural networks

1. sentiment classification: sequence of words -> sentiment, many -> one
2. machine translation: words -> seq of words, many -> many
3. video classification on frame level: many to many
4. truncated backpropagation
5. min-char-rnn, see [link](https://gist.github.com/karpathy/d4dee566867f8291f086)
6. the stacks project: open source algebraic geometry textbook, see [link](http://stacks.math.columbia.edu)
7. torvalds/linux
8. searching for interpretable cells [Karpathy, Johnson, Fei-Fei: Visualizing and Understanding Recurrent Networks, ICLR workshop 2016]
9. image captioning
10. **image captioning with attention: Show, attend and tell: neural image caption generation with visual attention, ICML2015 [Xu et al.]**
    * soft attention
11. visual question answering
    * VQA: visual question answering, ICCV2015 [Agrawal et al.]
    * Visual 7W: Grounded question answering in images, CVPR2016 [Zhu et al.]
12. Multilayer RNNs
13. LSTM(???): Long Short Term Memory [Hochreiter et al. 1997]
    * f: Forget gate
    * i: Input gate
    * g: Gate gate
    * o: Output gate
14. Vanilla RNN gradient flow
    * graident explode exponentially: graident clipping
    * vanishing gradient problem: change RNN architecture
15. **othere RNN variants**
16. GRU: Learning phrase representations using rnn encoder-decoder for statistical machine translation [Cho et al. 2014] (gate recurent unit)
17. LSTM: A search space odyssey [Greff et al. 2015]
18. An empirical exploration of Recurrent Network architectures [Jozefowicz et al. 2015]

## lecture 11 detectionn and segmentation

1. Net2Net [Lan Goodfellow]
2. network Morphism [Microsoft]
3. computer vision task
   * smantic segmentation (no objects, just pixels)
   * classification + localization (single object)
   * object detection (multi objects)
   * instance segmentation (multi objects)

semantic segmentation

1. sliding window: very inefficient
2. fully convolutional: compute expensive, huge memory
3. convolutional layers with downsampling and upsampling
   * Fully Convlutional networks for segmantic segmentation, CVPR2015 [Long Shelohamer and Darrell]
   * Learning Deconvolution network for Semantic Segmentation ICCV2015 [Noh et al.]
   * downsampling: stride, pooling
   * upsampling: unpooling, max unpooling, **Unet with conv inside**
   * learnable upsampling: Transpose convolution / Deconvolution / Upconvolution / Fractionally strided convolution / Backward strided convolution

classification + localization

1. treat localization as a regression problem: two loss function
2. hyperparameter: weight between two loss function: some other metric of performance
3. human pose estimation: 14 joint positions

object detection

1. each image needs a different number of outputs
2. slide window: apply CNN to huge number of locations and scales, very computationally expensive
3. region proposals / regions of interest / ROIs
   * find 'blobby' image regions that are likely to contain objects
   * relatively fast to run
   * selective search
4. R-CNN
   * selective search
   * warp to fixed size
   * CNN
   * SVMs
   * regression -> four numbers
   * softmax classifier (log loss) (???) / linear SVMs (hinge loss) / bounding-box regressions (least squares)
   * training is slow (84h), takes a lot of disk space
   * inference (detection) is slow: 47s/image with VGG16 [Simonyan & Zisserman ICLR2015]
   * fixed by SPP-net [He et al. ECCV2014]
5. Fast R-CNN
   * ConvNet
   * ROIs from a proposal method and map on feature maps
   * ROI pooling layer
   * FCs
   * Linear + softmax classifier
   * Linear (bounding-box regressors)
6. Faster R-CNN
   * Region Proposal Network (RPN)
   * 4 losses: RPN classify object / not object, RPN regress box coordinates / final classification score (object classes) / final box coordinates
7. You Look Only Once (YOLO), Single Shot Detection (SSD)
   * divide into course grid $7 \times 7$
   * base bounding box $B$
   * regress from each of the $B$ base boxes to a final box with 5 numbers (dx, dy, dh, dw, confidence)
   * predict scores for each of $C$ classes (including background)
   * output $7 \times 7 \times \left( 5\times B + C \right)$
8. object detection: lots of variables
   * base network: VGG16, ResNet-101, Inception V2, Inception V3, Inception, ResNet, MobileNet
   * object detection architecture: Faster R-CNN, R-FCN, SSD
   * Image Size: region proposals
   * Faster R-CNN is slower but more accurate
   * SSD is faster but not as accurate
   * Speed / accuracy trade-offs for modern convolutional object detector CVPR2017 [Huang et al.]
9. objection detection + captioning = dense captioning
   * DenseCap: Fully Convolutional Localization Networks for Dense captioning CVPR2016 [Johnson, Karpathy and Fei-Fei]

Instance Segmentation

1. Mask R-CNN
   * CNN
   * ROI align
   * much omitted
2. pose estimation with Mask R-CNN: Mask R-CNN [He et al.]

## lecture 12 visualizeing and understanding

1. first conv
2. last fully connection layer with nearest neighbor
3. last layer: dimensionality reduction
   * principle component analysis (PCA)
   * t-distributed stochastic neighbor embeddings (t-SNE)
   * cs.stanford.edu/people/karpathy/cnnembed
4. visualizing activations
5. maximally axtivating patches
6. Occlusion experiments: Visualizing and understanding convolutional networks [Zeiler and Fergus]
7. Salienccy maps: Deep inside convoluutional networks: visualizing Image classification models [Simonyan, Vedaldi and Zisserman] -> segmentation unlabeled (Grabcut segmentation algorithm)
8. intermediate features via (guided) backprop: find the part of an image that a neuron responds to
9. gradient ascent: generate a synthetic image that maximally activates a neuron
10. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks NIPS2016 [Nguyen et al.]
11. Fooling Images / Adversarial Examples
    * start from an arbitrary image
    * pick an arbitrary class
    * modify the image to maximize the class
    * repeat until network is fooled
12. DeepDream: amplify existing features, amplify the neuron activations at some layer in the network
    * choose an image and a layer in a CNN, repeat
    * forward: compute activations at chosen layer
    * set gradient of chosen layer equal to its activation
    * backward: compute gradient on image
    * update image
13. Feature inversion: given a CNN feature vector for an image, find a new image that, understanding deep image representations by inverting them, CVPR2015 [Mahendran and Vedaldi]
    * matches the given feature vector
    * looks natureal (image prior regularization)
    * total variation regularizer (encourages spatial smoothness)
14. texture synthesis: given a sample patch of some texture, generate a bigger image of the same texture
    * nearest neighbor
    * gram matrix, texture synthesis using convolutional neural networks NIPS2015 [Gatys, Ecker, and Bethge]
15. Neural Style Transfer, Image style transfer using convolutional neural networks CVPR 2016 [Gatys, Ecker, and Bethge]
    * requires many forward / backward passes through VGG; very slow
16. fast style transfer: perceptual losses for real-time style transfer and super-resolution ECCV2016 [Johnson, Alahi and Fei-Fei]
17. Fast Style Transfer, texture networks: feed-forward synthesis of textures and stylized images ICML2016 [Ulyanov et al.]
18. One network, many styles: A learned representation for artistic style ICLR2017 [Dumoulin, shlens, and Kudlur]

## lecture 13 generative models

```bash
generative models
--explicit density
----tractable density, fully visible belief nets, NADE, MADE, PixelRNN/CNN
----approximate density
------variational, variational autoencoder (VAE)
------Markov chain, Boltzmann machine
--implicit density
----direct, GAN
----Markov chain, GSN
```

1. clustering, dimensionality reduction (PCA), feature learning, density estimation,
2. generative models: address density estimation
   * explicit density estimation: explicitly define and solve for $\rho_{model} \left( x \right)$
   * implicit density estimation: learn model that can sample from $\rho_{model} \left( x \right)$
   * realistic samples for artwork, super-resolution, colorization
   * generative model of time-series data can be used for simulation and planning (reinforcement learning application)
   * training generative model can also enable inference of latent representatios that can be useful as general features
3. PixelRNN [van der Oord et al. 2016]
   * generate image pixels starting from the corner
   * dependency on previus pixels modeled using an RNN (LSTM)
   * slow
4. PixelCNN [van der Oord et al. 2016]
   * generate pixel starting from the corner
   * dependecy on previus pixels now modeled with a CNN over context region
   * faster than PixelRNN but still slow
   * explicityly compute likelyhood
   * explicit likelihood of training data gives good evaluation metric
   * good samples
   * sequential generation -> slow
   * improving PixelCNN: gated conv, short-cut connections, discretized logistic loss, multi-scale, training trick (Van der Oord et al. NIPS2016) (Salimans et al. 2017)
5. VAE, auto-encoding variational ICLR2014 [Kingma and Welling]
   * intractable density function with latent z
   * cannot optimize directly, derive and optimize lower bound on likelihood instead
   * autoencoder, used to initialize a supervised model
   * assume training data is generated from underlying unobserved (latent) representation z
   * 完全没听懂
   * probabilistic spin to traditional autoencoders -> allow generating data
   * define an intractable data -> derive and optimize lower bound
   * pros: principled approach to generateive models
   * pros: allow inference of q(z|x), can be useful feature representation for other tasks
   * cons: maximize lower bound of likelihood, okay but not as good evaluation as PixelRNN / PixelCNN
   * cons: samples blurrier and lower quality compared to state-of-the-art (GANs)
   * more flexible approximations: richer approximate posterior instead of diagonal Gaussian
   * incorporating structure in latent variables

generative adversial networks (GANs) NIPS2014 [Ian Goodfellow]

1. problem: want to sample from complex, high-dimensiaonal training distribution
2. solution: sample from a simple distribution, e.g. random noise. Learn transformation to training distribution
3. generator network: try to fool the discriminator by generating real-looking images
4. discriminator network: try to distinguish between real and fake images
5. minimax objective function
6. Wasserstein GAN
7. Unsupervised representation Learning with Deep convolutional generative adversial networks ICLR2016 [Radford et al.]
   * replace any pooling layers with sstrided convolutional (discriminator) and fractional-strided convolutional (generator)
   * use batchnorm in both the generator and the discriminator
   * remove fully connected hidden layers for deeper architecture
   * use ReLU activation in generator for all layers except for the output, which uses Tanh
   * use LeakyReLU activation in the discriminator for all layers
8. year of the GAN
   * LSGAN. [Mao et al. 2017]
   * BEGAN. [Bertholet et al. 2017]
   * CycleGAN. [Zhu et al. 2017]
   * [Akata et al. 2017]
   * Pix2pix Iscla 2017 many examples, see [link](https://phillipi.github.io/pix2pix/)
   * the GAN Zoo, see [link](https://github.com/hindupuravinash/the-gan-zoo)
   * tips and tricks for trainings GANs, see [link](https://github.com/soumith/ganhacks)
9. better loss function, more stable training (Wasserstein GAN, LSGAN, many others)
10. conditional GANs, GANs for all kinds of applications

## lecture 14 deep reinforcement learning

1. cart-pole problem
2. robot locamotion
   * objective: make the robot move forward
   * state: angle and position of the joints
   * action: torque applied on joints
   * reward: 1 at each time step upright + forward movement
3. atari games
   * complete the game with the highest score
   * state: raw pixel inputs of the game state
   * action: game controls, e.g. left / right / up / down
   * reward: score increase / decrease at each time step
4. DeepMinds AlphaGo beats Lee Sedol
   * objective: win the game
   * state: position of all pieces
   * action: where to put next piece down
   * reward: 1 if win the end of the game, 0 otherwise
5. Markov Decision Process (MDP)
   * current state completely characteristics the state of the world
   * $\mathcal{S,A,R,P,\gamma}$, set of possible states / set of possible actions / distribution of reward given (state, action) pair / transition probability, distribution over next state given (state, action) pair / discount factor
6. a policy $\pi$ is a function from $\mathcal{S}$ to $\mathcal{A}$ that specifies what action to take in each state
7. objective: find policy $\pi^*$ that maximizes cumulative discounted reward: $\sum\limits_{t>0} {\gamma^tr_t}$
8. value function
9. Q-value function
10. Bellman equaion
11. 完全听不懂
12. [Mnih et al.] NIPS2013 nature2015
13. omitted。。。卒

## lecture 15 efficient methods and harware for deep learning

algorithms for efficient inference

1. pruning neural networks
2. weight sharing [Han et al. ICLR16]
3. Huffman coding
4. SqueezeNet [Iandola et al.] SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5mb model size
5. quantization [Qiu et al.] going deeper with embedded FPGA platform for convolutional neural netowrk FPGA2016
6. low rank approximation [Novikov et al.] Tensorizing neural networks NIPS2015
7. Binary /Ternary Net [Zhu, Han, Mao Dally] Trained Ternary Quantization ICLR2017
8. Winograd transformation, [A. Lavin, B.S. Gray] fast algorithms for convolutional neural networks

harware for efficient inference

1. Eyeriss, MIT, RS Dataflow
2. DaDiannao, CAS, eDRAM
3. TPU, Google, 8-bit Integer
4. EIE, Stanford, Compression / Sparsity

algorithms for efficient training

1. data paralel
2. model parallel convolution, High performance hardware for machine learning NIPS2015 [Dally]
3. hyperparameter parallel
4. mixed precision with FP16 and FP32, [Boris Ginsburg, Sergei Nikolaev, Paulius Micikevicius] training with mixed precision NVIDIA GTC2017
5. model distillation [Hinton et al.] Dar knowledge / Distilling the knowledge in a neural network
6. Dense Sparse Dense Training (DSD) Dense-Sparse-Dense Training for Deep Neural Networks ICLR2017 [Han et al.]

hardware for efficient training

1. NVIDIA PASCAL GP100 (2016)
2. NVIDIA VOLTA GV100 (2017) tensor core
3. Google Cloud TPU
4. Xormad on raspberry pi

## lecture 16 adversrial examples and adversarial training [Ian Goodfellow]

ops
